{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6장 데이터 로딩과 저장, 파일 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_api' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\Flex\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m12345\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mrc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m\"\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      6\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mmax_colwidth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m75\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py:174\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\cbook.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _c_internal_utils\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_running_interactive_framework\u001b[39m():\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    Return the interactive framework whose event loop is currently running, if\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    any, or \"headless\" if no event loop can be started, or None.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m        \"macosx\", \"headless\", ``None``.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_api' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\Flex\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(12345)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc(\"figure\", figsize=(10, 6))\n",
    "pd.options.display.max_colwidth = 75\n",
    "pd.options.display.max_columns = 20\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "plt.rcParams[\"font.family\"] = 'Malgun Gothic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. csv 텍스트 데이터\n",
    "## 1.1 csv파일 읽기\n",
    "* 판다스에서 CSV 파일을 읽기: pd.read_csv(path, encoding = 'utf-8') 함수를 사용\n",
    "    * path 인수 : 파일시스템에서의 위치, URL, 파일객체를 나타내는 문자열\n",
    "    * encoding 인수 - 유니코드 인코딩 종류을 지정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) header(컬럼명)가 있는 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a,b,c,d,message\n",
      "1,2,3,4,hello\n",
      "5,6,7,8,world\n",
      "9,10,11,12,foo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c  d message\n",
       "0  1  2  3  4   hello\n",
       "1  5  6  7  8   world"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat examples/ex1.csv\n",
    "\n",
    "df = pd.read_csv(\"examples/ex1.csv\", encoding= \"utf-8\") # csv file -> DataFrame으로 반환\n",
    "type(df)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2) header가 없는 파일 : 컬럼을 지정\n",
    "        * names 인수 - 열 이름 리스트\n",
    "        * header 인수 - 헤더가 없을 경우 None으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat examples/ex2.csv\n",
    "\n",
    "names = [\"a\", \"b\", \"c\", \"d\", \"message\"]\n",
    "data = pd.read_csv(\"examples/ex2.csv\", names=names) #names 인수 - 데이터프레임의 컬럼명 지정\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3) 계층적 색인 지정하기\n",
    "        * index_col 인수 - 색인으로 사용할 열 번호나 이름, 계층적 색인을 지정할 경우 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat examples/csv_mindex.csv\n",
    "parsed = pd.read_csv(\"examples/csv_mindex.csv\",\n",
    "                     index_col=[\"key1\", \"key2\"])\n",
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4) 여러 개의 공백(spacebar)으로 필드를 구분한 파일 : txt포맷\n",
    "        * sep 인수 - 필드(컬럼)을 구분하기 위해 사용할 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat examples/ex3.txt\n",
    "result = pd.read_csv(\"examples/ex3.txt\", sep=\"\\s+\") # 정규표현식 : \" \\s+  \"\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5) 주석을 갖고 있는 파일\n",
    "        * skiprows 인수 - 파일의 시작부터 무시할 행 수 또는 무시할 행 번호가 담긴 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat examples/ex4.csv\n",
    "pd.read_csv(\"examples/ex4.csv\", skiprows=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6) 누락된 값이 있는 파일 : 비어 있는 문자열, NA, NULL을 NaN으로 출력 \n",
    "        * na_values 인수 - NA 값으로 처리할 값들의 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat examples/ex5.csv\n",
    "# na_values 인수을 사용하지 않은 경우\n",
    "result = pd.read_csv(\"examples/ex5.csv\")\n",
    "result\n",
    "# 1) 누락된 값을 확인 : isna(), isnull(), notna(), notnull()\n",
    "pd.isna(result)\n",
    "\n",
    "# 2) na_values -> 컬럼별 문자열 집합을 받아서 누락된 값 처리\n",
    "sentinels = {\"message\": [\"foo\", \"NA\"], \"something\": [\"two\"]}\n",
    "result = pd.read_csv(\"examples/ex5.csv\", na_values=sentinels)\n",
    "print(result)\n",
    "result.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7) 텍스트 파일 조금씩 \n",
    "        * nrows 인수 - 파일의 첫 일부만 읽어올 때 처음 몇 줄을 읽을 것인지 지정 \n",
    "        * chunksize 인수 - TextParser 객체에서 사용할 한 번에 익을 파일의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.options.display.max_rows = 10\n",
    "!cat examples/ex6.csv\n",
    "# result = pd.read_csv(\"examples/ex6.csv\")\n",
    "# result\n",
    "# result = pd.read_csv(\"examples/ex6.csv\", nrows=5)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제]  ex6.csv 파일을 순회하면서 \"key\"열의 값을 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = pd.read_csv(\"examples/ex6.csv\", chunksize=1000)\n",
    "chunker\n",
    "# type(chunker)\n",
    "#1) 리스트의 시리즈 생성 - 청크 다위로 key\"열의 값 저장\n",
    "tot = pd.Series([], dtype='int64')\n",
    "type(tot)\n",
    "#2) 반복 처리 \n",
    "\n",
    "for piece in chunker:\n",
    "    # print(type(piece[\"key\"]))\n",
    "    # print(piece[\"key\"].value_counts())\n",
    "    tot = tot.add(piece[\"key\"].value_counts(), fill_value=0)\n",
    "\n",
    "tot = tot.sort_values(ascending=False)\n",
    "tot\n",
    "tot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note\n",
    "from pandas import Series\n",
    "tot = pd.Series([], dtype='int64')\n",
    "# type(tot) # Series\n",
    "\n",
    "for piece in range(3):\n",
    "    tot = tot.add(Series([1,2,3]), fill_value = 10) # tot + Series([1,2,3]\n",
    "tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. csv 텍스트 파일 저장하기\n",
    "* 판다스에서 CSV 파일을 저장하기: df.to_csv(path, encoding = 'utf-8') 함수를 사용\n",
    "    * path 인수 : 파일시스템에서의 위치, URL, 파일객체를 나타내는 문자열\n",
    "    * encoding 인수 - 유니코드 인코딩 종류을 지정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat examples/ex5.csv\n",
    "# 1) 준비 : DataFrame\n",
    "data = pd.read_csv(\"examples/ex5.csv\") #-> DataFrame 반환\n",
    "# 2. 조작 : DataFrame에서 인덱싱해서 필요한 데이터 추출-> DataFrame\n",
    "data1 = data[['a','b','d']]\n",
    "print(data1)\n",
    "# # 3. data1 -> csv 파일로 저장\n",
    "data1.to_csv(\"examples/out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv(\"examples/out.csv\", sep=\"|\") # 구분자 사용\n",
    "data1.to_csv(\"examples/out.csv\", na_rep = \"NULL\") #누락된 값을 원하는 값으로 지정\n",
    "# data.to_csv(\"examples/out.csv\", index=False, header=False)\n",
    "data1.to_csv(\"examples/out.csv\", index=False, columns=[\"a\", \"b\", \"d\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. JSON  데이터\n",
    "* JSON(JavaScript Object Notation)은 텍스트 데이터를 저장하고 전송하기 위한 일반적인 형식\n",
    "* 주로 웹에서 데이터를 전송하거나 저장하는 데 사용\n",
    "* JSON 파일은 일반적으로 텍스트 형식으로 작성되며, 사람과 기계 모두 이해하기 쉽다.\n",
    "* JSON 파일은 키-값 쌍의 모음으로 구성\n",
    "  * 키-값 쌍은 객체(object)라고도 하며, 중괄호 {} 로 묶는다.\n",
    "  * 배열(array)은 대괄호 [] 안에 값의 목록을 나열하여 정의."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 JSON 데이터 읽기/저장\n",
    "    * pandas의 to_json() -> 데이터프레임을 JSON 파일로 저장\n",
    "    * read_json() -> JSON 파일에서 데이터프레임을 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제]  pandas의 to_json() 및 read_json() 메서드를 사용하여 데이터프레임을 JSON 파일로 저장하고 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. pandas 라이브러리 임포트\n",
    "import pandas as pd\n",
    "\n",
    "# 2. JSON 파일로 저장할 데이터프레임 생성\n",
    "data = {\n",
    "    \"name\": [\"John\", \"Alice\", \"Bob\"],\n",
    "    \"age\": [30, 25, 35],\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 3. 데이터프레임을 JSON 파일로 저장\n",
    "df.to_json(\"examples/data.json\")\n",
    "\n",
    "# # JSON 파일을 읽어와 데이터프레임으로 변환\n",
    "df_read = pd.read_json(\"examples/data.json\")\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nDataFrame from JSON file:\")\n",
    "print(df_read)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. HTML 데이터\n",
    "## 3.1 웹 스크래핑 - read_html()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* read_html() - html 문서 내 테이블 데이터 읽어 테이블 구조의 모든 데이터를 데이터프레임으로 저장해서 반환\n",
    "    * https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 개발자 도구 사용 방법\n",
    "    * #s_content > div.section > ul > li:nth-child(1) > dl > dt > a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [예제] 미국 예금 보험 공사에서 부도 은행을 보여주는 HTML 문서 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "tables = pd.read_html(\"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\")\n",
    "type(tables)\n",
    "len(tables) # 1\n",
    "failures = tables[0]\n",
    "failures\n",
    "type(failures)\n",
    "failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테이터프레임 가공\n",
    "# 1) 컬럼명 수정하기\n",
    "failures.columns\n",
    "failures.columns = ['BankNameBank', 'CityCity', 'StateSt', 'CertCert',\n",
    "       'AcquiringInstitutionAI', 'ClosingDate', 'FundFund']\n",
    "failures.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 데이터 변환 : 문자열 object -> datetime\n",
    "close_timestamps = pd.to_datetime(failures[\"ClosingDate\"]) # 컬럼 인덱싱\n",
    "close_timestamps\n",
    "#3) 년도별 부도난 은행의 개수 추출\n",
    "close_timestamps.dt.year.value_counts() # Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [실습] : 네이버 주식 데이터 가져오기 - read_html()\n",
    "* https://www.naver.com/ -> 증권 -> 국내증시 -> 시가총액\n",
    "    * https://finance.naver.com/sise/sise_market_sum.naver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install html5lib  lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'euc-kr' 또는 'cp949'로 인코딩 설정하여 페이지 읽기 시도\n",
    "df_list = pd.read_html(\"https://finance.naver.com/sise/sise_market_sum.naver\",encoding = 'cp949')\n",
    "type(df_list)\n",
    "len(df_list)\n",
    "type(df_list[1])\n",
    "# 데이터프레임 확인\n",
    "for df in df_list:\n",
    "    print(\"\\n\",df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임 가공- 종목명에 NaN이 읶는 행은 제거\n",
    "df = df_list[1] # 컬럼 인덱싱\n",
    "df.head(10)\n",
    "#boolean indexing \n",
    "df['종목명'] # 1. 컬럼 추출 -Series\n",
    "df['종목명'].notnull() #2. 조건 색인\n",
    "data = df[df['종목명'].notnull()] # 3. 조건색인으로 인덱싱하기\n",
    "data\n",
    "data.head()\n",
    "# 가공한 데이터프레임을 파일로 저장\n",
    "data.to_csv(\"examples/Naver_kospi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 웹 스크래핑 - requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 웹 페이지 정보 가져오기 - requests 패키지 이용\n",
    "  * requests.get()\n",
    "  * requests.get().status_code\n",
    "  * requests.get().text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제] 타겟 사이트의 json 구조로 저장된 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. requests 모듈 임포트하기\n",
    "import requests\n",
    "import pandas as pd\n",
    "#2. url 가져오기\n",
    "url = \"https://api.github.com/repos/pandas-dev/pandas/issues\"\n",
    "#pd.read_html(url)\n",
    "# 3.페이지을 가져오기\n",
    "resp = requests.get(url)\n",
    "type(resp)\n",
    "resp.status_code #200\n",
    "resp.text # 데이터 가져오기\n",
    "type(resp.text)\n",
    "\n",
    "# 4. json 내용을 딕셔너리로 변환\n",
    "data = resp.json() # json의 내용을 딕셔너리 형태로 변환한 객체을 반환\n",
    "type(data)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] 네이버 블로그의 키워드 관련 검색 결과 텍스트 데이터 가져오기 - requests 모듈 사용시 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# url = \"https://section.blog.naver.com/Search/Post.naver?\\\n",
    "# pageNo=1&rangeType=ALL&orderBy=sim&keyword=%ED%8C%8C%EC%9D%B4%EC%8D%AC\"\n",
    "\n",
    "params = {\n",
    "    'pageNo' : 1,\n",
    "    'rangeType' : 'ALL',\n",
    "    'orderBy' : 'sim',\n",
    "    'keyword' : '파이썬'\n",
    "}\n",
    "\n",
    "#pd.read_html(url)\n",
    "response = requests.get('https://section.blog.naver.com/Search/Post.naver?', params=params)\n",
    "\n",
    "print(response.status_code) # HTTP GET 방식으로 URL 파라미터 값 전달\n",
    "print(response.url) # status_code 는 응답코드\n",
    "print(response.text) # text에는 HTML 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 웹 스크래핑 - beautifulsoup (4/10일 비대면 수업)\n",
    "* 웹 페이지 정보 추출하기 - beautifulsoup 사용법 \n",
    "  * soup.select_one()\n",
    "  * soup.select()\n",
    "  * get_text()\n",
    "* 개발자 도구 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] 네이버 지식iN의 검색어 관련 검색 결과 제목 가져오기\n",
    "* 타겟 웹 사이트 : https://kin.naver.com/search/list.naver?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬이 미래에 도움이 될까요?\n",
      "파이썬 사용\n",
      "웹개발공부와 파이썬공부\n",
      "맥북 파이썬 idel\n",
      "파이썬 질문\n",
      "파이썬과 자바\n",
      "파이썬 질문...\n",
      "파이썬 배우기\n",
      "파이썬강좌 독학할때 뭐보나요??\n",
      "파이썬독학할 때 어떻게 공부를... \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#https://kin.naver.com/search/list.naver?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC\n",
    "url = 'https://kin.naver.com/search/list.naver?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    #웹페이지 확인\n",
    "    html = response.text\n",
    "    # html -> beautifulsoup 객체로 변환 :: 데이터를 더 쉽게 추출\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #5. 데이터추출 : 제목 한개 선택하여 텍스트 가져오기\n",
    "    \n",
    "    title = soup.select_one('#s_content > div.section > ul > li:nth-child(1) > dl > dt > a')\n",
    "    #print(title)\n",
    "    #print(title.get_text())\n",
    "\n",
    "    # 제목 여러 개 선택하여 텍스트 리스트 가져오기\n",
    "    titles = soup.select('#s_content > div.section > ul > li:nth-child(n) > dl > dt > a')\n",
    "    #print(titles)\n",
    "    #7. 제목의 텍스트를 반복적으로 적용하여 가져오기\n",
    "    for title in titles:\n",
    "        print(title.get_text())\n",
    "else : \n",
    "    print(response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 웹 스크래핑 - Selenium을 이용하여 사이트 자동화하기\n",
    "* selenium 사용법\n",
    "  1. 시스템의 OS 및 Chrome 브라우저의 버전 확인\n",
    "  2. Chrome 드라이버 다운로드 및 설치\n",
    "  3. 크롬 드라이버을 프로젝트 폴더에 저장 - 크롬 드라이버의 설치 경로\n",
    "     * from selenium import webdriver\n",
    "     * driver = webdriver.Chrome()\n",
    "* 동적으로 웹 페이지 정보 추출하기\n",
    "     * driver.get(\"웹주소\") \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제] Selenium을 이용하여 자동으로 타겟 웹 사이트 실행하기\n",
    "* 타겟 사이트 : https://www.google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.19.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\flex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.25.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\flex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\flex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\flex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\flex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\flex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\flex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\flex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\flex\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/10.5 MB 11.6 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.7/10.5 MB 9.1 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/10.5 MB 9.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/10.5 MB 8.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/10.5 MB 7.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.0/10.5 MB 9.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.4/10.5 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.6/10.5 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.0/10.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.3/10.5 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.8/10.5 MB 9.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.2/10.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.4/10.5 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.9/10.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.2/10.5 MB 9.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.6/10.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.9/10.5 MB 9.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.3/10.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.5/10.5 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.9/10.5 MB 9.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.3/10.5 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.8/10.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.2/10.5 MB 9.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.5 MB 9.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.8/10.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.2/10.5 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.6/10.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.9/10.5 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.4/10.5 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 9.6 MB/s eta 0:00:00\n",
      "Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "   ---------------------------------------- 0.0/467.2 kB ? eta -:--:--\n",
      "   --------------------------------------  460.8/467.2 kB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 467.2/467.2 kB 9.7 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, pysocks, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.19.0 sortedcontainers-2.4.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://selenium-python.readthedocs.io/\n",
    "from selenium import webdriver\n",
    "# Chrome 브라우저 실행\n",
    "driver = webdriver.Chrome()\n",
    "# 웹페이지 열기\n",
    "url = \"https://www.google.com\"\n",
    "driver.get(url)\n",
    "\n",
    "#3. 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] Selenium을 이용하여 자동으로 타겟 웹 사이트 실행하기/검색하기/검색 결과 출력하기\n",
    "* 타겟 사이트: https://www.google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목: OpenAI\n",
      "URL: https://openai.com/\n",
      "\n",
      "제목: OpenAI\n",
      "URL: https://openai.com/\n",
      "\n",
      "제목: OpenAI\n",
      "URL: https://namu.wiki/w/OpenAI\n",
      "\n",
      "제목: OpenAI\n",
      "URL: https://en.wikipedia.org/wiki/OpenAI\n",
      "\n",
      "제목: OpenAI\n",
      "URL: https://www.youtube.com/@OpenAI\n",
      "\n",
      "제목: OpenAI\n",
      "URL: https://twitter.com/OpenAI\n",
      "\n",
      "제목: OpenAI\n",
      "URL: https://www.linkedin.com/company/openai\n",
      "\n",
      "제목: 설명\n",
      "URL: https://www.google.com/search?q=OpenAI&sca_esv=f7e7867956c72750&source=hp&ei=5TUWZqG9NcWavr0Pu-a8sA4&iflsig=ANes7DEAAAAAZhZD9X0uZxG6BaV_KPyfLhfhq-uRn46v&udm=&ved=0ahUKEwjhw6u_hreFAxVFja8BHTszD-YQ4dUDCBA&uact=5&oq=OpenAI&gs_lp=Egdnd3Mtd2l6IgZPcGVuQUlIPlAAWBhwAHgAkAEAmAEAoAEAqgEAuAEDyAEA-AEBmAIAoAIAmAMAkgcAoAcA&sclient=gws-wiz#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Chrome 브라우저 실행\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Google 홈페이지 열기\n",
    "url = \"https://www.google.com\"\n",
    "driver.get(url)\n",
    "\n",
    "# 페이지가 완전히 로드될 때까지 2초 대기. . .\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    # 검색 상자 요소 찾기\n",
    "    search_box = driver.find_element(By.NAME, \"q\") \n",
    "\n",
    "    # 검색어 입력\n",
    "    search_query = \"OpenAI\"\n",
    "    search_box.send_keys(search_query) #여기까지 검색어 입력 하는 행동\n",
    "\n",
    "    # Enter 키를 눌러 검색 수행\n",
    "    search_box.send_keys(Keys.RETURN) #리턴 = 엔터\n",
    "\n",
    "    # 검색 결과가 로드될 때까지 5초 대기\n",
    "    time.sleep(5)\n",
    "\n",
    "   \n",
    "    search_results = driver.find_elements(By.CSS_SELECTOR, \"div.g\") \n",
    "     # 검색 결과 추출 - 이런 방법도 잇어\n",
    "    # title = search_result[0].find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "    # url = serach_result[0].find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "    # print(\"제목:\", title)\n",
    "    # print(\"URL:\", url)\n",
    "\n",
    "    # 검색 결과의 제목과 URL 인쇄\n",
    "    for result in search_results:\n",
    "        title = result.find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "        url = result.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "        print(\"제목:\", title)\n",
    "        print(\"URL:\", url)\n",
    "        print()\n",
    "\n",
    "    # 검색에 관련된 결과 추출\n",
    "    #search_results = driver.find_elements(By.CSS_SELECTOR, \"div.g\") \n",
    "    \n",
    "    # title = search_results[0].find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "    # url = search_results[0].find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "    # print(\"제목:\", title)\n",
    "    # print(\"URL:\", url)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"오류가 발생했습니다:\", e)\n",
    "\n",
    "finally:\n",
    "    # 브라우저 창 닫기\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] Selenium을 이용하여 네이버 증권 웹페이지 열기/ 삼성전자 일별 시세 HTML가져오기/ 10페이지 내역 테이블 데이터 찾기/검색 결과를 데이터프레임으로 변환/저장하기\n",
    "* 타겟 사이트: https://finance.naver.com/item/sise.naver?code=005930* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_9896\\1569354950.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블구조 -> 데이터프레임\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_9896\\1569354950.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블구조 -> 데이터프레임\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_9896\\1569354950.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블구조 -> 데이터프레임\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "1   2024.04.09  83600.0   900.0  84500.0  84900.0  83100.0  18342168.0\n",
      "2   2024.04.08  84500.0     0.0  85200.0  86000.0  84500.0  18953232.0\n",
      "3   2024.04.05  84500.0   800.0  84500.0  85000.0  83800.0  18883752.0\n",
      "4   2024.04.04  85300.0  1200.0  85200.0  85500.0  84300.0  25248934.0\n",
      "5   2024.04.03  84100.0   900.0  84300.0  85000.0  83500.0  30493347.0\n",
      "9   2024.04.02  85000.0  3000.0  82900.0  85000.0  82900.0  37077944.0\n",
      "10  2024.04.01  82000.0   400.0  83200.0  83300.0  82000.0  20116513.0\n",
      "11  2024.03.29  82400.0  1600.0  81200.0  82500.0  80900.0  27126366.0\n",
      "12  2024.03.28  80800.0  1000.0  79400.0  81000.0  79200.0  25084812.0\n",
      "13  2024.03.27  79800.0   100.0  79200.0  80000.0  79200.0  17424595.0,             날짜       종가     전일비       시가       고가       저가         거래량\n",
      "1   2024.03.26  79900.0  1700.0  79700.0  80100.0  79200.0  30551494.0\n",
      "2   2024.03.25  78200.0   700.0  79600.0  79800.0  77800.0  18703996.0\n",
      "3   2024.03.22  78900.0   400.0  79600.0  79900.0  77800.0  26724761.0\n",
      "4   2024.03.21  79300.0  2400.0  79200.0  79300.0  77700.0  44569799.0\n",
      "5   2024.03.20  76900.0  4100.0  73700.0  77200.0  73400.0  50106297.0\n",
      "9   2024.03.19  72800.0     0.0  72300.0  73000.0  71700.0  15376066.0\n",
      "10  2024.03.18  72800.0   500.0  72600.0  73000.0  72500.0  11520348.0\n",
      "11  2024.03.15  72300.0  2000.0  73400.0  73700.0  72300.0  22580555.0\n",
      "12  2024.03.14  74300.0   200.0  74400.0  74500.0  73600.0  22545539.0\n",
      "13  2024.03.13  74100.0   800.0  73700.0  74100.0  73500.0  15243134.0,             날짜       종가     전일비       시가       고가       저가         거래량\n",
      "1   2024.03.12  73300.0   900.0  72600.0  73500.0  72100.0  13011654.0\n",
      "2   2024.03.11  72400.0   900.0  72900.0  73100.0  72300.0   9740504.0\n",
      "3   2024.03.08  73300.0  1100.0  72800.0  73400.0  72600.0  19271349.0\n",
      "4   2024.03.07  72200.0   700.0  73100.0  73300.0  72200.0  14516963.0\n",
      "5   2024.03.06  72900.0   800.0  73200.0  73500.0  72700.0  21547905.0\n",
      "9   2024.03.05  73700.0  1200.0  74600.0  74800.0  73700.0  19505125.0\n",
      "10  2024.03.04  74900.0  1500.0  74300.0  75000.0  74000.0  23210474.0\n",
      "11  2024.02.29  73400.0   200.0  72600.0  73400.0  72000.0  21176403.0\n",
      "12  2024.02.28  73200.0   300.0  72900.0  73900.0  72800.0  11795859.0\n",
      "13  2024.02.27  72900.0   100.0  73100.0  73400.0  72700.0  13201981.0,             날짜       종가     전일비       시가       고가       저가         거래량\n",
      "1   2024.02.26  72800.0   100.0  72300.0  73200.0  72200.0  14669352.0\n",
      "2   2024.02.23  72900.0   200.0  73600.0  74200.0  72900.0  16225166.0\n",
      "3   2024.02.22  73100.0   100.0  73800.0  73900.0  72700.0  15208934.0\n",
      "4   2024.02.21  73000.0   300.0  73400.0  73700.0  72900.0  11503495.0\n",
      "5   2024.02.20  73300.0   500.0  73700.0  73700.0  72800.0  14681477.0\n",
      "9   2024.02.19  73800.0  1000.0  72800.0  73900.0  72800.0  12726404.0\n",
      "10  2024.02.16  72800.0   200.0  73300.0  73400.0  72500.0  13444781.0\n",
      "11  2024.02.15  73000.0  1000.0  74200.0  74400.0  73000.0  14120600.0\n",
      "12  2024.02.14  74000.0  1200.0  73700.0  74300.0  73700.0  12434945.0\n",
      "13  2024.02.13  75200.0  1100.0  74800.0  75200.0  74400.0  21966745.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_9896\\1569354950.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(html) # 테이블구조 -> 데이터프레임\n"
     ]
    }
   ],
   "source": [
    "#1. 크롬 드라이버 실행\n",
    "driver= webdriver.Chrome()\n",
    "\n",
    "# 2페이지 로딩 시간\n",
    "time.sleep(2)\n",
    "\n",
    "# 3. 크롤링 사이트 : 삼성전자의 일별 시세 페이지 URL 찾기\n",
    "## https://finance.naver.com/item/sise_day.naver?code=005930&page=1\n",
    "samsung_url =\"https://finance.naver.com/item/sise_day.naver?code=005930&page=\" #base url\n",
    "\n",
    "\n",
    "# 4. 첫 페이지부터 5 페이지까지 데이블 담기\n",
    "samsung_daily_kospi = [] # 리스트 선언\n",
    "for num in range(1,5):\n",
    "    full_url = samsung_url + str(num) #스트링에 문자열 붙이려면 오류가 나니까 중간에 수정\n",
    "\n",
    "    # 4.1 해당 페이지 요청\n",
    "    driver.get(full_url)\n",
    "    \n",
    "    try:\n",
    "        # 페이지의 HTML 가져오기\n",
    "        html = driver.page_source\n",
    "        df = pd.read_html(html) # 테이블구조 -> 데이터프레임\n",
    "        # 결측치 처리 - 조건색인으로 불리언 인덱싱\n",
    "        ##조건색인인덱싱\n",
    "        cond = df[0]['날짜'].notnull()\n",
    "        samsung_day = df[0][cond]\n",
    "        #리스트에 담기\n",
    "        samsung_daily_kospi.append(samsung_day)       \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Network Error\",e)\n",
    "    \n",
    "    # finally:\n",
    "    #     driver.quit()\n",
    "\n",
    "print(samsung_daily_kospi)\n",
    "samsung_daily_kospi_all = pd.concat(samsung_daily_kospi) #데이터프레임 합치기\n",
    "samsung_daily_kospi_all.to_csv(\"examples/samsung_daily_kospi_2024_2020.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
